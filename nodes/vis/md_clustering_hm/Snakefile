from tempfile import NamedTemporaryFile

import pandas as pd
import altair as alt
import numpy as np
import jinja2 as j2

import altair_saver
import re
import json
import os

from nodes.vis.md_clustering_hm.scripts.utils import cluster

TOKEN = config["token"]

rule all:
    input:
         config["html_dir_out"] + "md_clustering_hm.json"

rule overview_data:
    input:
        config["metric_dirs_in"]
    output:
        temp(f"data/temp/{TOKEN}/hm_dendro_data.csv")
    run:
        paths = [p + "f1.csv" for p in list(input)]

        df_res = pd.DataFrame()
        for p in paths:
            df = pd.read_csv(p, index_col=0)
            df_medians = df.apply(np.median).to_frame("median")
            group = lambda enc: \
                "psekraac" if "lambda-corr" in enc or "g-gap" in enc else enc[:6]
            df_medians["group"] = [group(x) for x in df_medians.index]

            df_tmp = df_medians.groupby(by="group").max()
            df_tmp.columns = [re.findall("data/(.*?)/", p)[0]]
            df_res = pd.concat([df_res, df_tmp], axis=1)

        hm_tmp = df_res.fillna(0.0).copy(deep=True)

        row_indices = cluster(hm_tmp.values, axis=0)
        col_indices = cluster(hm_tmp.values, axis=1)
        heatmap_data = df_res.iloc[row_indices, col_indices]

        heatmap_data.to_csv(output[0])

rule make_dendro_data:
    input:
         f"data/temp/{TOKEN}/hm_dendro_data.csv"
    output:
         config["html_dir_out"] + "hm_dendro_{axis}_data.json"
    script:
         "scripts/compute_dendrogram.R"

rule cluster_hm_data:
    input:
         f"data/temp/{TOKEN}/hm_dendro_data.csv"
    output:
         config["html_dir_out"] + "hm_dendro_data_hm_values_data.json",
         temp(f"data/temp/{TOKEN}/hm_dendro_data_0_transform.json"),
         temp(f"data/temp/{TOKEN}/hm_dendro_data_1_transform.json")
    run:
         heatmap_data = pd.read_csv(input[0], index_col=0)
         x, y = np.meshgrid(heatmap_data.columns, heatmap_data.index)

         source = pd.DataFrame({"Dataset": x.ravel(),
                                "Encoding": y.ravel(),
                                "F1": heatmap_data.values.ravel(),
                                "sort_ds_idx": range(len(x.ravel())),
                                "sort_en_idx": range(len(y.ravel()))
                                })

         # source.to_json(output[0], orient="records")

         source["missing"] = source["F1"].apply(lambda f1: True if np.isnan(f1) else False)

         RECT_SIZE = 13

         sort_x = alt.Sort(alt.SortArray(heatmap_data.columns.to_list()))
         sort_y = sort=alt.Sort(alt.SortArray(heatmap_data.index.to_list()))

         tooltip = ["Encoding:N", "Dataset:N", "F1:Q", "is_imbalanced:Q"]

         chart1 = alt.Chart(source).mark_rect().encode(
             y=alt.Y('Encoding:N',sort=sort_y),
             x=alt.X('Dataset:N', sort=sort_x),
             color=alt.Color('F1:Q', scale=alt.Scale(
                 domain=[0.0, 1.0], range=["#a6bddb", "#023858"]
             )),
             tooltip=tooltip
         )

         chart2 = alt.Chart(source).mark_rect(size=RECT_SIZE).encode(
             y=alt.Y('Encoding:N', sort=sort_y),
             x=alt.X('Dataset:N', axis=alt.Axis(labelAngle=-45), sort=sort_x),
             color=alt.Color(
                 'F1_new:N',
                 title="Value",
                 scale=alt.Scale(domain=["NA"], range=["#a6611a"])
             )
         ).transform_calculate(
             F1_new="datum.F1 == null ? 'NA' : 'NA'"
         ).transform_filter(
             alt.datum.F1 == None
         ).properties(
             height={"step": RECT_SIZE},
             width={"step": RECT_SIZE}
         )

         chart = (chart1 + chart2)

         with NamedTemporaryFile("w", dir=f"data/temp/{TOKEN}/", suffix=".vg.json") as f:
             altair_saver.save(chart, f.name)
             os.remove("geckodriver.log")
             with open(f.name) as json_in, \
                     open(output[0], "w") as data_hm_out, \
                     open(output[1], "w") as data_0_out, \
                     open(output[2], "w") as data_1_out:
                 vg_spec = json.load(json_in)
                 import pydevd_pycharm
                 pydevd_pycharm.settrace('localhost', port=4000, stdoutToServer=True, stderrToServer=True)
                 vg_spec["data"][0]["name"] = "data-hm"
                 vg_spec["data"][1]["source"] = "data-hm"
                 json.dump(vg_spec["data"][0]["values"], data_hm_out),
                 json.dump(vg_spec["data"][1]["transform"], data_0_out),
                 json.dump(vg_spec["data"][2]["transform"], data_1_out)

rule aggregate_hm_cluster:
    input:
         expand(config["html_dir_out"] + "hm_dendro_{axis}_data.json",
                axis=["row", "col"]),
         config["html_dir_out"] + "hm_dendro_data_hm_values_data.json",
         f"data/temp/{TOKEN}/hm_dendro_data_0_transform.json",
         f"data/temp/{TOKEN}/hm_dendro_data_1_transform.json"
    output:
         config["html_dir_out"] + "md_clustering_hm.json"
    run:
         env = j2.Environment(
             loader=j2.FileSystemLoader("nodes/vis/md_clustering_hm/templates/"),
             autoescape=j2.select_autoescape(["json"])
         )

         with open(input[0]) as f0, open(input[1]) as f1, \
             open(input[2]) as f2, open(input[3]) as f3, open(input[4]) as f4:
             template = env.get_template("hm_cluster.json")
             template\
                .stream(values_row_dendro=json.load(f0),
                        values_col_dendro=json.load(f1),
                        values_hm_values=json.load(f2),
                        data_0_transform=json.load(f3),
                        data_1_transform=json.load(f4))\
                .dump(output[0])
