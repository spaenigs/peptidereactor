from itertools import combinations, product
from pathos.multiprocessing import ProcessingPool as Pool
from scipy import interpolate, stats
from sklearn.manifold import TSNE
from glob import glob

import pandas as pd
import numpy as np

import re
import os

TOKEN = config["token"]

CSVS = glob(config["csv_in"] + "*.csv")

def generate_input_files(psekraac_type):
    return [p for p in CSVS if f"{psekraac_type}_st" in p]

rule all:
    input:
         f"data/temp/{TOKEN}/psekraac_filtered.txt"

# rule correlation:
#     input:
#          lambda wildcards: generate_input_files(wildcards.psekraac_type)
#     output:
#          f"data/temp/{TOKEN}/{{psekraac_type}}_distance_matrix_tmp.alt.csv"
#     threads:
#          1000
#     run:
#          def interpolate_to(X: np.array, dim: int):
#              ydim = X.shape[1]
#              x = np.arange(0, ydim if ydim >= 2 else ydim + 1)
#              func = interpolate.interp1d(x, X, kind="linear")
#              return func(np.linspace(0, ydim - 1, dim))
#
#          # TODO https://deepgraph.readthedocs.io/en/latest/
#
#          # maybe use kruskal wallis?
#          def f(csv_paths):
#              csv_path_1, csv_path_2 = csv_paths
#              df1 = pd.read_csv(csv_path_1, index_col=0).iloc[:, :-1]
#              df2 = pd.read_csv(csv_path_2, index_col=0).iloc[:, :-1]
#              to_dim = int(np.ceil(np.mean([df1.shape[1], df2.shape[1]])))
#              ds1_interpolated, ds2_interpolated = interpolate_to(df1, to_dim), interpolate_to(df2, to_dim)
#              corr, _ = stats.pearsonr(ds1_interpolated.ravel(), ds2_interpolated.ravel())
#              return pd.DataFrame({"e1": [csv_path_1], "e2": [csv_path_2], "R": [corr]})
#
#          p = Pool(workflow.cores)
#          csv_paths = list(input)
#          print(csv_paths)
#          df_res = pd.concat(p.map(f, combinations(csv_paths, 2)))
#          df_res.to_csv(output[0])

rule correlation_2:
    input:
         lambda wildcards: generate_input_files(wildcards.psekraac_type)
    output:
         temp(f"data/temp/{TOKEN}/{{psekraac_type}}_distance_matrix_tmp.csv")
    threads:
         1000
    script:
        "scripts/correlation.R"

rule corr_matrix:
    input:
         f"data/temp/{TOKEN}/{{psekraac_type}}_distance_matrix_tmp.csv",
         lambda wildcards: generate_input_files(wildcards.psekraac_type)
    output:
         temp(f"data/temp/{TOKEN}/{{psekraac_type}}_distance_matrix.csv")
    run:
          df = pd.read_csv(input[0], index_col=0)
          df.set_index(["e1", "e2"], inplace=True)

          csv_paths = input[1:]
          csv_products = list(zip(*product(csv_paths, repeat=2)))
          df_res = pd.DataFrame({"e1": csv_products[0], "e2": csv_products[1], "R": 0.0})
          df_res["R"] = df_res.apply(lambda row: 1.0 if row[0] == row[1] else row[2], axis=1)
          df_res.set_index(["e1", "e2"], inplace=True)

          for i in df.index:
              df_res.loc[i, "R"] = df.loc[i, "R"]

          matrix = df_res.reset_index().pivot(index="e1", columns="e2", values="R")

          # move all values to lower side
          matrix_tmp = np.tril(np.triu(matrix, 1).transpose() + matrix.values)

          # copy lower part to upper part (matrix is now symmetric)
          matrix_tmp = np.tril(matrix_tmp, -1) + np.tril(matrix_tmp).transpose()

          # 1 - abs(values) to get distance matrix, 0 == equal
          matrix = pd.DataFrame(1 - np.abs(matrix_tmp), index=matrix.index, columns=matrix.columns)

          matrix.to_csv(output[0])

rule run_pca:
    input:
         f"data/temp/{TOKEN}/{{psekraac_type}}_distance_matrix.csv"
    output:
         temp(f"data/temp/{TOKEN}/{{psekraac_type}}_pca.csv")
    run:
         matrix = pd.read_csv(input[0], index_col=0)

         embedding = TSNE(n_components=2, metric="precomputed")
         X_transformed = embedding.fit_transform(matrix)

         df_for_cls = pd.DataFrame(X_transformed, index=matrix.index, columns=["x1", "x2"])
         df_for_cls.to_csv(output[0])

rule clustering:
    input:
         f"data/temp/{TOKEN}/{{psekraac_type}}_pca.csv"
    output:
         temp(f"data/temp/{TOKEN}/{{psekraac_type}}_filtered.csv")
    script:
         "scripts/clustering.R"

def get_files(wildcards):
    psekraac_types = []
    for p in CSVS:
        hits = re.findall("(t\w+)_st.*", os.path.basename(p))
        if len(hits) > 0:
            psekraac_types += [hits[0]]
    res = \
        expand(f"data/temp/{TOKEN}/{{psekraac_type}}_filtered.csv", psekraac_type=set(psekraac_types))
    return res

rule collect:
    input:
         get_files
    output:
         temp(f"data/temp/{TOKEN}/psekraac_filtered.txt")
    run:
         pattern = ".*/t.*?_st.*rt.*ktu.*la.*"

         non_psekraac_files = \
             [p for p in CSVS if re.search(pattern, p) is None]

         for p in non_psekraac_files:
             shell(f"cp {p} {config['csv_out']}")

         for p in list(input):
             source_file = pd.read_csv(p, index_col=0).iloc[0, 0]
             shell(f"cp {source_file} {config['csv_out']}")

         shell("touch {output[0]}")