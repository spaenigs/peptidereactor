from sklearn.decomposition import PCA
from pathos.multiprocessing import ProcessingPool as Pool
from more_itertools import chunked
from functools import partial

import pandas as pd
import numpy as np

TOKEN = config["token"]

rule all:
    input:
         f"data/temp/{TOKEN}/aaindex_filtered.txt"

rule filter_low_correlated_indices:
    input:
         "peptidereactor/iFeature/data/AAindex.txt"
    output:
         temp(f"data/temp/{TOKEN}/low_correlated.csv")
    run:
         aaindex = pd.read_csv(str(input), sep="\t", index_col=0)
         aaindex.columns = aaindex.columns[1:].tolist() + ["NaN"]
         aaindex = aaindex.iloc[:, :-1].transpose()

         X = aaindex.corr()

         pca = PCA(n_components=1)
         X_new = pca.fit_transform(X)

         # Encodings with a big separation after PCA, should also have a low correlation.
         # Conversely, encodings with a low separation after PCA should also have a high correlation.
         encoding_dist = pd.Series([x[0] for x in X_new], index=aaindex.columns)
         encoding_dist = ((encoding_dist - encoding_dist.min())/(encoding_dist.max()-encoding_dist.min()))
         encoding_dist[(0.47 < encoding_dist) & (encoding_dist < 0.53)].to_csv(str(output))

rule filter_files:
    input:
         f"data/temp/{TOKEN}/low_correlated.csv",
         config["csv_in"]
    output:
         temp(f"data/temp/{TOKEN}/aaindex_filtered.txt")
    run:
         aaindex = pd.read_csv(str(input[0]), index_col=0)

         def filter_files(from_files, target_dir):

             # copy all files to target dir, except aaindex related
             aaindex_files = []
             for p in from_files:
                 if "aaindex" in p:
                    aaindex_files += [p]
                 else:
                    shell(f"cp {p} {target_dir}")

             # filter and copy non-correlated aaindex files to target dir
             for idx in aaindex.index:
                 for f in aaindex_files:
                     if idx in f:
                        shell(f"cp {f} {target_dir}")

             # for csv_path in from_files:
             #     df = pd.read_csv(csv_path, index_col=0, engine="c")
             #     nrows, ncols = df.shape
                 # https://academic.oup.com/bioinformatics/article/21/8/1509/249540#2950870
                 # if ncols - 1 <= nrows:
                 #         shell(f"cp {csv_path} {target_dir}")

         def run(from_files, target_dir):
             cores = workflow.cores
             p = Pool(cores)
             chunk_len = int(np.round(len(from_files)/cores))
             chunks = chunked(from_files, chunk_len)
             pfunc = partial(filter_files, target_dir=target_dir)
             p.map(pfunc, chunks)

         run(list(input), config["csv_out"])

         shell("touch {output}")













