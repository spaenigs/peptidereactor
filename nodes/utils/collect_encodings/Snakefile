from pathos.multiprocessing import ProcessingPool as Pool
from more_itertools import chunked
from functools import partial

import pandas as pd
import numpy as np

TOKEN = config["token"]


def filter_files(from_files, target_dir):
    for csv_path in from_files:
        shell(f"cp {csv_path} {target_dir}")


def run(from_files, target_dir):
    cores = workflow.cores
    p = Pool(cores)
    chunk_len = int(np.round(len(from_files)/cores))
    chunks = chunked(from_files, chunk_len)
    pfunc = partial(filter_files, target_dir=target_dir)
    p.map(pfunc, chunks)


rule all:
    input:
         f"data/temp/{TOKEN}/sequence_based_encodings_collected.txt",
         f"data/temp/{TOKEN}/structure_based_encodings_collected.txt"

rule collect_sequence_based_encodings:
    input:
         config["csv_seq_in"]
    output:
         temp(f"data/temp/{TOKEN}/sequence_based_encodings_collected.txt")
    threads:
         1000
    run:
         run(list(input), config["csv_seq_out"])
         shell("touch {output[0]}")

rule collect_structure_based_encodings:
    input:
         config["csv_str_in"]
    output:
         temp(f"data/temp/{TOKEN}/structure_based_encodings_collected.txt")
    threads:
         1000
    run:
         run(list(input), config["csv_str_out"])
         shell("touch {output[0]}")