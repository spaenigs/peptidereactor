import joblib as jl
from Bio import SeqIO


rule all:
    input:
         config["output_file_0"],
         config["output_file_1"],
         config["output_file_2"],
         config["output_file_3"]
        # expand("data/{dataset}_{part}.fasta",
        #        dataset=config["dataset"], part=["ds1", "ds2"]),
        # expand("data/{dataset}_{part}_classes.txt",
        #        dataset=config["dataset"], part=["ds1", "ds2"])


rule init:
    input:
        config["input_file_0"],
        config["input_file_1"]
    output:
        temp("data/interim.fasta"),
        temp("data/interim_classes.txt")
    shell:
        """
        cp {input[0]} {output[0]}
        cp {input[1]} {output[1]}
        """


rule read_fasta:
    input:
        "data/interim.fasta"
         # "data/{dataset}.fasta"
    output:
        temp("data/interim_from_fasta.joblib")
    run:
        raw_data, seq_names = [], []
        for record in SeqIO.parse(str(input), "fasta"):
            seq_names.append(record.name)
            raw_data.append([record.name, str(record.seq)])
        if len(set(seq_names)) != len(raw_data):
            raise Exception("Seqence names should be unique!")
        jl.dump(value=raw_data, filename=str(output))


rule read_classes:
    input:
         "data/interim_classes.txt"
        # "data/{dataset}_classes.txt"
    output:
        temp("data/interim_from_classes.joblib")
    run:
        f = open(str(input), mode="r")
        target = [int(line.rstrip()) for line in f.readlines()]
        jl.dump(value=target, filename=str(output))


rule create_input_data:
    input:
        "data/interim_from_fasta.joblib",
        "data/interim_from_classes.joblib"
    output:
        temp("data/interim.joblib")
    run:
        from_fasta = jl.load(filename=str(input[0]))
        from_classes = jl.load(filename=str(input[1]))
        sorted_in_da = [tup for tup in sorted(zip(from_fasta, from_classes), key=lambda tup: tup[0][0])]
        from_fasta_sorted = list(map(lambda tup: tup[0], sorted_in_da))
        from_classes_sorted = list(map(lambda tup: tup[1], sorted_in_da))
        input_data = (from_fasta_sorted, from_classes_sorted)
        jl.dump(value=input_data, filename=str(output))


rule create_normal_distributed_input_data:
    input:
        "data/interim.joblib"
    output:
        temp("data/interim_normal_distributed.joblib"),
        temp("data/interim_ds1_normal_distributed.joblib"),
        temp("data/interim_ds2_normal_distributed.joblib"),
    script:
        "scripts/create_normal_distributed_input_data.py"


rule save_as_fasta:
    input:
         "data/interim_{part}_normal_distributed.joblib"
    output:
        "data/interim_{part}.fasta",
        "data/interim_{part}_classes.txt"
    run:
        from modlamp.core import save_fasta
        def create_fasta(input_data_):
            seq_tups = input_data_[0]
            classes = input_data_[1]
            seq_names = list(map(lambda tup: tup[0], seq_tups))
            seqs = list(map(lambda tup: tup[1], seq_tups))
            return seq_names, seqs, classes
        input_data = jl.load(str(input))
        n, s, c = create_fasta(input_data)
        save_fasta(str(output[0]), sequences=s, names=n)
        with open(str(output[1]), mode="w") as f:
            for c_ in c:
                f.write(str(c_) + "\n")
                f.flush()

rule collect:
    input:
         lambda wildcards: expand("data/interim_{part}.fasta", part=["ds1", "ds2"]) + \
                           expand("data/interim_{part}_classes.txt", part=["ds1", "ds2"])
    output:
         config["output_file_0"],
         config["output_file_1"],
         config["output_file_2"],
         config["output_file_3"]
    shell:
        """
        cp {input[0]} {output[0]}
        cp {input[1]} {output[1]}
        cp {input[2]} {output[2]}
        cp {input[3]} {output[3]}
        """