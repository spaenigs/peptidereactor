from Bio import SeqIO
from Bio.Blast.Applications \
    import NcbiblastpCommandline
from Bio.PDB import PDBParser
from modlamp.core import save_fasta, read_fasta
from io import StringIO
from glob import glob

import pandas as pd

import os
import re

from nodes.utils.tertiary_structure_search.scripts.dssp_parser \
    import generate_disorder_profile, generate_spinex_profile
from nodes.utils.tertiary_structure_search.scripts.utils \
    import get_seq_names, Cif

include:
    "setup_in_structure_db.smk"

TOKEN = config["token"]

CIFS_DIR = "peptidereactor/db/cifs/"
PDB_DIR = config["pdb_dir"]
PROFILE_DIR = config["profile_dir"]

rule all:
    input:
         config["fasta_sec_out"],
         config["classes_sec_out"],
         config["fasta_ter_out"],
         config["classes_ter_out"]

rule split_input_data:
    input:
         config["fasta_in"]
    output:
         f"data/temp/{TOKEN}/{{seq_name}}.fasta"
    run:
         seqs, names = read_fasta(str(input[0]))
         seq_tuples = dict((name, seq) for name, seq in zip(names, seqs))
         seq = seq_tuples[wildcards.seq_name]
         save_fasta(str(output), sequences=[seq], names=[wildcards.seq_name])

rule blast_search:
    input:
         f"data/temp/{TOKEN}/{{seq_name}}.fasta",
         "peptidereactor/db/pdb/in_structure/pdb.db"
    output:
         f"data/temp/{TOKEN}/blast_result_{{seq_name}}.csv"
    priority:
        1000
    run:
         header = ["qseqid", "sacc", "sstart", "send", "evalue", "qseq", "sseq"]
         cline = NcbiblastpCommandline(
             task="blastp-short",
             db=str(input[1]),
             query=str(input[0]),
             evalue=200000, # include distinct hits
             max_target_seqs=1,
             max_hsps=1,
             outfmt="10 " + " ".join(header))
         stdout, stderr = cline()

         df_res = pd.read_csv(StringIO(stdout), names=header)
         blast_hits = df_res.shape[0]

         if blast_hits > 0:

             df_res["sacc"] = \
                 df_res["sacc"].apply(lambda full_id: full_id.split("_not_by_pdb_")[0])

             ids, chains = \
                 zip(*df_res["sacc"].apply(
                     lambda full_id: full_id.split("_")).values.tolist())
             df_res["sacc_id"] = [id.lower() for id in ids]
             df_res["sacc_chain"] = chains

             # remove hits with two-letter chain names (see https://stackoverflow.com/questions/50579608)
             df_res = \
                 df_res.loc[ [False if len(id) > 1 else True for id in df_res["sacc_chain"]]
                           , :]

         df_res.to_csv(str(output))

rule motif_search:
    input:
         f"data/temp/{TOKEN}/blast_result_{{seq_name}}.csv",
         f"data/temp/{TOKEN}/{{seq_name}}.fasta",
         "peptidereactor/db/pdb/in_structure/pdb.fasta"
    output:
         f"data/temp/{TOKEN}/motif_result_{{seq_name}}.csv"
    run:
         df = pd.read_csv(str(input[0]), dtype={"sacc_id": str})

         if df.empty:
             seqs, names = read_fasta(input[1])
             seq, name = seqs[0], names[0]
             cmd = f"cat {input[2]} | grep {seq} -B 1"
             handle = os.popen(cmd)\
                 .read()\
                 .rstrip()\
                 .replace("--", "")
             df_res = pd.DataFrame()
             for r in SeqIO.parse(StringIO(handle), "fasta"):
                 pdb_id, chain_id = r.id.split("_not_by_pdb_")[0].split("_")
                 if len(chain_id) > 1:
                     continue
                 start = str(r.seq).find(seq)
                 end = start + 8
                 df_tmp = pd.DataFrame({
                     "qseqid": [name], "sacc": [r.id],
                     "sstart": [start], "send": [end], "evalue": [-1],
                     "qseq": [seq], "sseq": [seq],
                     "sacc_id": [pdb_id.lower()], "sacc_chain": [chain_id]})
                 df_res = pd.concat([df_res, df_tmp])
             df_res.to_csv(output[0])

         else:
             df.to_csv(output[0])

rule download_from_pdb:
    input:
         f"data/temp/{TOKEN}/motif_result_{{seq_name}}.csv"
    output:
         f"data/temp/{TOKEN}/cifs_downloaded_for_{{seq_name}}.txt"
    params:
         snakefile="nodes/utils/download_cifs/Snakefile",
         configfile="nodes/utils/download_cifs/config.yaml"
    threads:
        1000
    run:
         df = pd.read_csv(str(input[0]), dtype={"sacc_id": str})

         if not df.empty:
             id = df.loc[0, "sacc_id"]
             target_path = CIFS_DIR + f"{id}.cif"
             cmd = f"""
                 set +e; # escape strict mode to handle rsync error
                 id={id};
                 folder_id="${{{{id:1:2}}}}";
                 target_dir=$(dirname {target_path});
                 rsync -rlpt -v -z -q --delete --port=33444 \
                     rsync.rcsb.org::ftp_data/structures/divided/mmCIF/$folder_id/$id.cif.gz \
                     $target_dir #2> /dev/null;
                 if [ "$?" -eq "0" ]; then
                     gunzip {target_path}.gz;
                 else
                     echo "########## $id ####### $? ";
                     touch {target_path} 
                 fi
                 """
             if not os.path.exists(CIFS_DIR):
                 os.mkdir(CIFS_DIR)
             if not os.path.exists(target_path):
                 shell(cmd)

         shell("touch {output}")

rule dump_cleaved_structure:
    input:
         f"data/temp/{TOKEN}/motif_result_{{seq_name}}.csv",
         f"data/temp/{TOKEN}/cifs_downloaded_for_{{seq_name}}.txt"
    output:
         PDB_DIR + "{seq_name}.pdb"
    run:
         df = pd.read_csv(str(input[0]), dtype={"sacc_id":str})

         if df.empty:
             shell("touch {output}")
         else:
             pdb_id, chain_id, hit = \
                 df.loc[0, ["sacc_id", "sacc_chain", "sseq"]]

             cif = Cif(pdb_id, chain_id, CIFS_DIR)
             cif.dump_slice(hit, output[0])

rule collect_pdbs:
    input:
         expand(PDB_DIR + "{seq_name}.pdb", seq_name=get_seq_names(config["fasta_in"]))
    output:
         f"data/temp/{TOKEN}/pdb_collected.text"

rule collect_tertiary_structure:
    input:
         config["fasta_in"],
         config["classes_in"],
         expand(PDB_DIR + "{seq_name}.pdb", seq_name=get_seq_names(config["fasta_in"]))
    output:
         config["fasta_ter_out"],
         config["classes_ter_out"]
    run:
         seqs, names = read_fasta(input[0])
         with open(input[1]) as f:
             classes = [l.rstrip() for l in f.readlines()]

         seq_tuples = dict((name, tup) for name, tup in zip(names, zip(seqs, classes)))

         annotated_seqs, annotated_names, annotated_classes = [], [], []
         for pdb_path in list(input[2:]):
             if os.path.getsize(pdb_path) > 4:
                 seq_name = os.path.basename(pdb_path).replace(".pdb", "")
                 annotated_names += [seq_name]
                 annotated_seqs += [seq_tuples[seq_name][0]]
                 annotated_classes += [seq_tuples[seq_name][1]]

         save_fasta(output[0], annotated_seqs, annotated_names)

         with open(output[1], mode="a") as f:
             for c in annotated_classes:
                 f.write(f"{c}\n")
                 f.flush()

rule collect_secondary_structure:
    input:
         config["fasta_ter_out"],
         config["classes_ter_out"]
    output:
         directory(f"data/temp/{TOKEN}/dssps/"),
         config["fasta_sec_out"],
         config["classes_sec_out"]
    run:
         dssp_dir = output[0]

         seqs, names = read_fasta(str(input[0]))
         with open(str(input[1])) as f:
             classes = list(map(lambda l: int(l.rstrip()), f.readlines()))
         seq_tuples = dict((name, tup) for name, tup in zip(names, zip(seqs, classes)))

         def get_seq(pdb_id, pdb_path):
             structure = PDBParser().get_structure(pdb_id, pdb_path)
             chain_id = list(structure.get_chains())[0].get_id()
             cif = Cif(pdb_id, chain_id, PDB_DIR, file_type="pdb")
             return cif.seq

         seqs_out, names_out, classes_out = [], [], []
         for pdb_path in glob(PDB_DIR + f"/*.pdb"):
             if os.path.getsize(pdb_path) > 4:
                 pdb_id = re.findall(".*?/(\w+)\.pdb", pdb_path)[0]
                 path_to_dssp = f"{dssp_dir}/{pdb_id}.dssp"
                 path_to_dis = f"{PROFILE_DIR}/{pdb_id}.dis"
                 path_to_spXout = f"{PROFILE_DIR}/{pdb_id}.spXout"
                 try:
                     shell(f"mkdssp -i {pdb_path} -o {path_to_dssp}")
                 except Exception as e:
                     shell("touch {path_to_dis}")
                     shell("touch {path_to_spXout}")
                 else:
                     seqs_out += [get_seq(pdb_id, pdb_path)]
                     names_out += [pdb_id]
                     classes_out += [seq_tuples[pdb_id][1]]
                     generate_disorder_profile(path_to_dssp, path_to_dis)
                     generate_spinex_profile(path_to_dssp, path_to_spXout)

         save_fasta(output[1], seqs_out, names_out)

         with open(output[2], "w") as f:
             for c in classes_out:
                 f.write(f"{c}\n")
                 f.flush()
