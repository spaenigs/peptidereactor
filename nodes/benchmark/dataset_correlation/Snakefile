from glob import glob
from itertools import combinations
from more_itertools import chunked

import pandas as pd

import yaml
from numpy.distutils.system_info import dfftw_info

TOKEN = config["token"]

FILES = \
    glob(config["group_1_in"] + "*.csv") + \
    glob(config["group_2_in"] + "*.csv")

FILES_PRODUCT_1, FILES_PRODUCT_2 = \
    zip(*combinations(FILES, 2))

FROMs, TOs = \
    zip(*[(c[0], c[-1]+1) for c in chunked(range(len(FILES_PRODUCT_1)), 200)])

rule all:
    input:
         config["dataset_corr_out"]

rule split_batches:
    output:
         f"data/temp/{TOKEN}/batch_{{fr}}_{{to}}.yaml"
    run:
         fr, to = int(wildcards.fr), int(wildcards.to)

         res = []
         for f1, f2 in zip(FILES_PRODUCT_1[fr:to], FILES_PRODUCT_2[fr:to]):
            res += [[f1, f2]]

         with open(output[0], "w") as f:
             yaml.safe_dump(res, f)

rule compute_dataset_correlation:
    input:
         f"data/temp/{TOKEN}/batch_{{fr}}_{{to}}.yaml"
    output:
         f"data/temp/{TOKEN}/dataset_correlation_{{fr}}_{{to}}.csv"
    params:
         cores=workflow.cores
    script:
         "scripts/dataset_correlation.R"

rule collect:
    input:
         expand(f"data/temp/{TOKEN}/dataset_correlation_{{fr}}_{{to}}.csv",
                 zip, fr=FROMs, to=TOs)
    output:
         config["dataset_corr_out"]
    run:
         df_res = pd.DataFrame()
         for p in list(input):
             df_res = pd.concat([df_res, pd.read_csv(p, index_col=0)])

         df_res.index = range(0, df_res.shape[0])
         df_res.to_csv(output[0])
