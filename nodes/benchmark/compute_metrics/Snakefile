from sklearn.metrics import \
    matthews_corrcoef, confusion_matrix, f1_score, precision_score, recall_score
from glob import glob

import pandas as pd
import numpy as np

import os
import re

# https://en.wikipedia.org/wiki/Phi_coefficient
# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html


def phi_score(tn, fp, fn, tp):
    return (tn*tp - fn*fp) / np.sqrt((tn+fn) * (fp+tp )* (fn+tp) * (tn+fp))


def collect_metrics(input_lst, out_path):
    df_res = pd.DataFrame()
    for p in input_lst:
        df_tmp = pd.read_csv(p, index_col=0)
        df_res = pd.concat([df_res, df_tmp])
    df_res.transpose().to_csv(out_path)


TOKEN = config["token"]

CSV_DIR = config["csv_dir_in"]
CSVS_IN = glob(config["csv_dir_in"] + "y_true_cv_*.csv")
NAMES = [os.path.basename(p).replace(".csv", "") for p in CSVS_IN]

METRICS_DIR = config["metrics_dir_out"]

rule all:
    input:
         expand(f"{METRICS_DIR}{{metric}}.csv",
                metric=["f1", "recall", "precision", "mcc", "sens", "spec", "phi"])

rule compute_metric:
    input:
         f"{CSV_DIR}{{name}}.csv"
    output:
         temp(f"data/temp/{TOKEN}/{{metric}}/{{name}}.csv")
    run:
         df_true = pd.read_csv(input[0], index_col=0)
         df_pred = pd.read_csv(input[0].replace("_true_", "_pred_"), index_col=0)

         m = wildcards.metric

         res = []
         for i in range(50):
             y_true = df_true.iloc[i, :].dropna()
             y_pred = df_pred.iloc[i, :].dropna()
             tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
             if m == "f1":
                 res += [f1_score(y_true, y_pred)]
             elif m == "recall":
                 res += [recall_score(y_true, y_pred)]
             elif m == "precision":
                 res += [precision_score(y_true, y_pred)]
             elif m == "mcc":
                 res += [matthews_corrcoef(y_true, y_pred)]
             elif m == "sens":
                 res += [tp / (tp + fn)]
             elif m == "spec":
                 res += [tn / ( tn + fp)]
             elif m == "phi":
                 res += [phi_score(tn, fp, fn, tp)]
             else:
                 raise ValueError(f"Unknown metric: {m}!")

         n = re.findall(".*y_true_cv_(.*).csv", input[0])[0]
         df_res = pd.DataFrame({n: res})
         df_res.transpose().to_csv(output[0])

rule collect:
    input:
         lambda wildcards: \
             expand(f"data/temp/{TOKEN}/{wildcards.metric}/{{name}}.csv", name=NAMES)
    output:
         f"{METRICS_DIR}{{metric}}.csv"
    run:
         collect_metrics(list(input), output[0])
