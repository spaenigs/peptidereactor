from sklearn.metrics import \
    classification_report, matthews_corrcoef, confusion_matrix
from glob import glob

import pandas as pd
import numpy as np

import os
import re

# https://en.wikipedia.org/wiki/Phi_coefficient
# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html


def phi_score(tn, fp, fn, tp):
    n = tn + fp + fn + tp
    n_11 = tn
    n_1_ = tp + fn
    n__1 = tp + fp
    return (n*n_11 - n_1_*n__1) / np.sqrt(n_1_*n__1 * (n-n_1_) * (n-n__1))


def collect_metrics(input_lst, out_path):
    df_res = pd.DataFrame()
    for p in input_lst:
        df_tmp = pd.read_csv(p, index_col=0)
        df_res = pd.concat([df_res, df_tmp])
    df_res.transpose().to_csv(out_path)


TOKEN = config["token"]

CSV_DIR = config["csv_dir_in"]
CSVS_IN = glob(config["csv_dir_in"] + "y_true_cv_*.csv")
NAMES = [os.path.basename(p).replace(".csv", "").replace("y_true_cv_", "")
         for p in CSVS_IN]

METRICS_DIR = config["metrics_dir_out"]

rule all:
    input:
         expand(f"{METRICS_DIR}{{metric}}.csv",
                metric=["f1", "recall", "precision", "mcc", "sens", "spec", "phi"])

rule compute_metric:
    input:
         f"{CSV_DIR}{{name}}.csv"
    output:
         f"data/temp/{TOKEN}/{{metric}}/{{name}}.csv",
    run:
         df_true = pd.read_csv(input[0], index_col=0)
         df_pred = pd.read_csv(input[0].replace("_true_", "_pred_"), index_col=0)

         m = wildcards.metric

         res = []
         for i in range(50):
             y_true = df_true.iloc[i, :].dropna()
             y_pred = df_pred.iloc[i, :].dropna()
             clf_report = classification_report(y_true, y_pred, output_dict=True)
             if m == "f1":
                 res += [clf_report["weighted avg"]["f1-score"]]
             elif m == "recall":
                 res += [clf_report["weighted avg"]["recall"]]
             elif m == "precision":
                 res += [clf_report["weighted avg"]["precision"]]
             elif m == "mcc":
                 res += [matthews_corrcoef(y_true, y_pred)]
             elif m == "sens":
                 res += [clf_report["1"]["recall"]]
             elif m == "spec":
                 res += [clf_report["0"]["recall"]]
             elif m == "phi":
                 tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
                 res += [phi_score(tn, fp, fn, tp)]
             else:
                 raise ValueError(f"Unknown metric: {m}!")

         n = re.findall(".*y_true_cv_(.*).csv", input[0])
         df_res = pd.DataFrame({n: res})
         df_res.transpose().to_csv(output[0])

rule collect:
    input:
         lambda wildcards: \
             expand(f"data/temp/{TOKEN}/{wildcards.metric}/{{name}}.csv", name=NAMES)
    output:
         f"{METRICS_DIR}{{metric}}.csv"
    run:
         collect_metrics(list(input), output[0])
